# Efficiently Serving LLM

In this repo we will start with token generation for a single request using GPT-2 from hugging face. Then we will delve into handling batch requests using batching and continous batching. We will also see techniques like quantization and multi-LoRA inference.